
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <title>How I Think About Instrumental Variables - The Good, The Bad And The Ugly.</title>
        <style>
            body {
                font-family: monospace;
                line-height: 1.6;
                font-size: 13px;
                margin: 0 auto;
                max-width: 800px;
                padding: 20px;
                letter-spacing: 0px;
            }

            /* Heading Styles */
            h1 {
                font-size: 24px;
                line-height: 1.2;
                margin-top: 40px;
                margin-bottom: 20px;
            }
            
            h2 {
                font-size: 20px;
                line-height: 1.2;
                margin-top: 30px;
                margin-bottom: 15px;
            }
            
            h3 {
                font-size: 27px;
                line-height: 1.2;
                margin-top: 25px;
                margin-bottom: 12px;
            }
            
            h4 {
                font-size: 15px;
                line-height: 1.2;
                margin-top: 20px;
                margin-bottom: 10px;
            }
            
            header {
                border-bottom: 1px solid #ccc;
                margin-bottom: 20px;
            }
            h1, h2, h3 {
                margin-top: 30px;
            }
            a {
                color: #0066cc;
                text-decoration: none;
            }
            a:hover {
                text-decoration: underline;
            }
            .post-date {
                color: #666;
                font-size: 0.9em;
            }
            .post-content {
                margin-top: 20px;
            }
            .quote {
                background-color: #f0f0f0;
                padding: 15px;
                border-left: 5px solid #2c3e50;
                margin: 20px 0;
            }
            .quote .quote {
                margin-left: 0;
                margin-right: 0;
            }
            li {
                padding: 2px 0;
            }
            em {
                font-style: italic;
            }
            strong {
                font-weight: bold;
            }
            /* MathJax styling */
            .MathJax {
                font-size: 1.1em !important;
            }
            .MathJax_Display {
                margin: 1em 0 !important;
                padding: 0.5em 0;
            }
        </style>
    </head>
    <body>
        <header>
            <h1>How I Think About Instrumental Variables - The Good, The Bad And The Ugly.</h1>
            <nav>
                <a href="index.html">Back to Main Page</a>
            </nav>
        </header>
        <main>
            <article>
                <p class="post-date">December 04, 2024</p>
                <div class="post-content">
                    <p>The whole point of Instrumental variables regression is to estimate the causal effect of an <em>unobservable variable</em> that if NOT included would result in OVB.</p>
<p>Essentially, you want to estimate the causal effect of $X$ on $Y$, given the estimated coefficient of $\beta_1$.</p>
<p>i.e. $$Y_i = \alpha + \beta_1 X_i + \epsilon_i$$
However, for whatever reason there is correlation between the explanatory variable $X$ and the error term $\epsilon$.
$$
\begin{aligned}
Y_i \: \: = \: \: \alpha \: \:+ \: \:\beta_1 X_i \: \: + \: \: \epsilon_i \\ \quad \overset{}{\nwarrow} \quad \overset{}{\nearrow} \\ \text{correlation}
\end{aligned}
$$
This will most likely occur due to Omitted Variable Bias, but it could also occur from data collection errors, etc. We can assume that more often the not, it will be due to OVB. In the case of OVB, we want to use an <strong>instrument</strong>, as that will help us find the true causal effect. Suppose, our new instrument is given as $Z$.</p>
<p>$Z$ <em>must</em> be <strong>uncorrelated</strong> with $\epsilon$, but still correlate with $X$ and can only influence $Y$ through $X$. THUS, our instrument is <strong>exogenous</strong>, as it is causes $X$, but has no effect on $Y$ other than acting through $X$.
$$\begin{aligned}
Z_i \: \: \rightarrow \: \: X_i    \: \: \rightarrow \:  Y_i  \: \\  \uparrow \quad  \nearrow \:\:\:\:\:\: \:\\ \epsilon_i \quad \: \: \:\:\:\:\:\:\: \
\end{aligned}$$</p>
<p>We can then essentially generate two regressions:
$$Y=\alpha + \beta X_i + \epsilon_i$$
$$\underbrace{X_i}_\text{total variation} = \underbrace{\mu \: \:+ \: \: \rho Z_i}_{\text{explained variation}=\hat{X}_i} \quad \: \:+ \underbrace{\eta_i}_\text{unexplained variation}$$
based on the second equation, we can see that the $\text{explained variation}$ is <strong>exogenous</strong> to our original equation because it depends on the exogenous variable $Z_i$ only. So in this sense, we split up our $X_i$ into a part that we can claim is certainly exogenous (the part that depends on $Z_i$) and some unexplained part $\eta_i$ that keeps all the bad variation which correlates with $\epsilon_i$. Now we take the exogenous part of this regression, and call it $\hat{X}_i$.</p>
<p>We can put this into our original regression:
$$Y_i = \alpha + \beta_1 (\mu + \rho Z_i) + \epsilon_i$$
$$Y_i = \beta_0 + \beta_1 (\hat{X_i}) + \epsilon_i$$
Since $\hat{X}_i$ is no longer correlated with $\epsilon_i$ (as we filtered out the correlated parts and left it in $\eta_i$) we can consistently estimate our $\beta$ because the instrument has helped us break the correlation between the explanatory variable and the error. Basically, $X_i$ is poisoned by unobserved factors in $\epsilon$ and we are simply cleaning/sucking that poison out when we calculate our instrument.</p>
<p>This process is known as the 2SLS or 2-Stage Least Squares method, where the "first stage" is $X_i = \mu + \rho Z_i = \hat{X}_i + \eta_i$ , and our "second stage" is $Y_i = \beta_0 + \beta_1 (\hat{X_i}) + \epsilon_i$.</p>
<p>In terms of our original Directed Acyclic Graph, instead of us taking the flawed route between $X$ and $Y$, we take an intermediate route via $\hat{X}_i$.</p>
<p>$$\begin{aligned} \hat{X}_i \\ \nearrow \quad \:  \downarrow \: \: \\
    Z_i \: \: \rightarrow \: \: X_i    \: \: \rightarrow \:  Y_i  \: \\  \uparrow \quad  \nearrow \:\:\:\:\:\: \:\\ \epsilon_i \quad \: \: \:\:\:\:\:\:\: \
    \end{aligned}$$
More cleanly,
$$\begin{aligned} \hat{X}_i \\ \nearrow \quad \:  \downarrow \: \: \\
Z_i \: \: \rightarrow \: \: X_i    \: \: \xcancel{\rightarrow} \:  Y_i
\end{aligned}$$
Thanks to this diversion in our DAG, we are able to consistently estimate $\beta$ by using the instrument. However, this generally results in our models being slightly less precise, and generally have higher standard errors.</p>
<p>I took inspiration from this <a href="https://stats.stackexchange.com/questions/563/what-is-an-instrumental-variable">Stack Exchange post</a>, for my diagrams, as I found this method more graspable then majority of the DAGs found in many econometrics textbooks.</p>
<hr />
<h3 id="simple-derivation-of-iv-estimator">Simple Derivation of IV Estimator</h3>
<p>We can also derive $\beta$ manually. Above I used $Z$ to represent the $\text{instrument}$,  $X$ to represent the $\text{treatment}$, and $Y$ to represent the $\text{outcome}$, for the below examples I will simply use the names.</p>
<p>We start off with two regression models:</p>
<p>$$\text{outcome}_i = \alpha + \beta \times \text{treatment}_i + \epsilon_i$$
$$\text{treatment}_i = \mu + \rho \times \text{instrument}_i + \eta_i$$
Kind of confusingly, the first model where <em>treatment</em> affects <em>outcome</em> is known as the <strong>second-stage model</strong> and the second model where the <em>instrument</em> affects the <em>treatment</em> is known as the <strong>first-stage model</strong>. This makes more sense if you think of the models in the above sections where the second stage model, is really the model where we input the treatment calculations $(Y_i = \alpha + \beta_1 [\mu + \rho Z_i] + \epsilon_i)$.</p>
<p>Anyway, once we calculate the first-stage regression, we can essentially approximate the $\text{treatment}$ value by removing the $\eta_i$ (error term).</p>
<p>Thus, $$\text{treatment}_i \approx \mu + \rho \times \text{instrument}_i$$
We substitute that back into the second-stage regression to get:
$$\text{outcome}_i = \alpha + \beta \times \text{treatment}_i + \epsilon_i$$</p>
<p>$$\approx \alpha + \beta \times (\mu +\rho \times \text{instrument}) + \epsilon_i$$
Expanding that out, we get:
$$= \alpha + \beta \mu + \beta \rho \times \text{instrument}_i + \epsilon_i$$
We simplify this down so that, $\kappa = \alpha +\beta\mu$ and $\lambda=\beta \rho$. Thus, because we know that $\lambda$ is simply equal to $\beta \rho$ and not some fancy equation like $\kappa$, we can simply divide he $\beta \rho$ by $\rho$ to get the value of $\beta$ on its own*:
$$\frac{\lambda}{\rho}=\frac{\beta \rho}{\rho} = \beta$$
*Assuming that $\rho \neq 0$ .</p>
<h3 id="weak-instrument">Weak Instrument</h3>
<p>The two-stage least squares (2SLS) method is capable of estimating the causal effect when an instrument fulfils three critical conditions. However, in practical applications, an additional implicit, yet crucial, requirement must be met for 2SLS to function effectively: <strong>the relevance condition must be sufficiently strong</strong>. â‡’ This means that the coefficient $\rho$, should be sufficiently different from zero. </p>
<div class="quote">
<p>If $\rho$ is too close to zero, the instrument in question is called a <strong>weak instrument</strong>.</p>
</div>
<p>If an instrument is weak, it can cause computational difficulties when estimating $\beta$ by dividing by $\rho$, because it effectively resembles a division by a near-zero value.</p>
<p>To determine whether $\rho$ is sufficiently distant from zero, a statistical test known as the <strong>F-test</strong> is performed on the first-stage regression, given that $\rho$ is a coefficient from this regression. The instrument is considered sufficiently strong, and thus not weak, <strong>if the F-test exceeds 10</strong>.</p>
<h2 id="mathematical-derivation-of-the-iv-estimator">Mathematical Derivation of the IV Estimator</h2>
<hr />
<p>Remember out two equations from above:
    <ol>
        <li><strong>First Stage</strong>: $X_i = \mu + \rho Z_i = \hat{X}_i + \eta_i$</li>
        <li><strong>Second Stage</strong>: $Y_i = \beta_0 + \beta_1 (\hat{X_i}) + \epsilon_i$</li>
        </ol></p>
<p>For each $i$ observation, we assume two important assumptions:
    <ol>
        <li><strong>Exogeneity</strong>: $C(Z,\epsilon)=0$</li>
        <li><strong>Relevance</strong>: $\rho \neq 0$</li>
        </ol>
These are based upon the idea of strong instruments. 
The following math is based on the expansions found in the <a href="https://mixtape.scunning.com/07-instrumental_variables#homogeneous-treatment-effects">Causal Mixtape</a>.</p>
<p>To determine our population IV estimator, we can start with our Exogeneity assumption, and determine that $E[Z_i\times \epsilon_i]=0$.</p>
<p>We can then rearrange our <em>Second Stage</em> equation to make $\epsilon_i$ as our dependent variable:
$$\epsilon_i=Y_i-\beta_0-\beta_1X_i$$
Thus, subbing this in to our expected value of the exogeneity assumption to get:
$$E[Z_i\times(Y_i-\beta_0-\beta_1X_i)]=0$$
Which if we expand out, by distributing our $Z_i$, we get:
$$E[Z_iY_i]-\beta_0E[Z_i]-\beta_1 E[Z_iX_i]=0$$
If our sample mean of $Z_i$ is zero, i.e. $\bar{Z}=0$, then $E[Z_i]=0$, which gets rid of our $\beta_0$ term, we can then rearrange our equation to isolate $\beta_1$ on the LHS, and simplifying to get:
$$\beta_1=\frac{E[Z_i Y_i]}{E[Z_iX_i]}$$
This gives us the population IV estimator. However, it is rare that we know the population, as such we often have to estimate the IV estimator based on sample averages.</p>
<p>We can then use sample approximations for the above expected values, which allows us to calculate the $\beta_1$ estimator $\hat{\beta}$:
$$E[Z_iY_i]\approx\frac{1}{n}\sum^n_{i=1}(Z_i-\bar{Z})(Y_i-\bar{Y})$$
$$E[Z_iX_i]\approx\sum^n_{i=1}(Z_i-\bar{Z})(X_i-\bar{X})$$
Thus, we can substitute this into our covariance equation to get:
$$\hat{\beta_1}=\frac{\frac{1}{n}\sum^n_{i=1}(Z_i-\bar{Z})(Y_i-\bar{Y})}{\frac{1}{n}\sum^n_{i=1}(Z_i-\bar{Z})(X_i-\bar{X})}$$
$$\hat{\beta_1}=\frac{\frac{1}{n}\sum^n_{i=1}(Z_i-\bar{Z})Y_i}{\frac{1}{n}\sum^n_{i=1}(Z_i-\bar{Z})X_i}$$
If we then substitute the true model for $Y_i$, we get the following:
$$\hat{\beta_1}=\frac{\frac{1}{n}\sum^n_{i=1}(Z_i-\bar{Z}){\alpha+\beta X_i+\epsilon_i}}{\frac{1}{n}\sum^n_{i=1}(Z_i-\bar{Z})X_i}$$
$$=\hat{\beta_1} + \frac{\frac{1}{n}\sum^n_{i=1}(Z_i-\bar{Z})\epsilon_i}{\frac{1}{n}\sum^n_{i=1}(Z_i-\bar{Z})X_i}$$
$$=\hat{\beta_1} + \text{"small if $n$ is large"}$$
But what about the estimator for $\rho$, well we can treat $\beta$ coefficient simply as a ratio of Covariances.
If we take our definition for covariances as:
$$C(Z,Y)=C(Y,Z)=E[ZY]-E[Z]E[Y]$$
$$C(Z,X)=C(X,Z)=E[ZX]-E[Z]E[X]$$
Since $E[Z]=0$ under our assumption that $\bar{Z}=0$, these simplify to:
$$C(Z,Y)=E[ZY]$$
$$C(Z,X)=E[ZX]$$
Which means that our $\beta_1$ estimator becomes:
$${\beta_1}=\frac{C(Z,Y)}{C(Z,X)}$$
So, let's return to our first description of $\hat{\beta}$ as the ratio of two covariances.
$$\hat{\beta_1}=\frac{C(Z,Y)}{C(Z,X)}$$
We can then make this estimation slightly more interpretable by standardising both the numerator and the denominator. If we divide by the variance of the instrument $V(Z)$, making the $\hat{\beta}_1$ coefficient interpretable in terms of how much change in $Y$ is expected for a change in $X$ that is induced by changes in the instrument $Z$:
$$\hat{\beta}_1=\frac{\frac{C(Z,Y)}{V(Z)}}{\frac{C(Z,X)}{V(Z)}}$$</p>
<p>We can then introduce our $\rho$ coefficient (the coefficient for our instrumental variable), which is equal to the ratio of covariance between $Z$ and $X$ standardised by our $Z$:
$$\hat{\rho}=\frac{C(Z,X)}{V(Z)}$$
We can see that this is also simply equal to our denominator in our above $\hat{\beta}_1$. </p>
<p>We can rewrite $\hat{\rho}$ as:
$$\hat{\rho}\cdot V(Z)=C(Z,X)$$
Thus we can substitute in to get our IV estimator:
$$\hat{{\beta}_{\text{1(IV)}}}=\frac{C(Z,Y)}{C(Z,X)}$$
We can then multiply both sides by $\hat{\rho}$:
$$=\frac{\hat{\rho}\cdot C(Z,Y)}{\hat{\rho}\cdot C(Z,X)}$$
Since, we know that $C(Z,X)= \hat{\rho}\cdot V(Z)$ we can sub into the equation and simplify to get:
$$=\frac{\hat{\rho}\cdot C(Z,Y)}{\hat{\rho}^2\cdot V(Z)}$$
$$=\frac{C(\hat{\rho Z}, Y)}{V(\hat{\rho}Z)}$$
Notice now what is inside the parentheses: $\hat{\rho}Z$, which are the fitted values from the first-stage of the regression. We are thus, no longer, using $X$, we are using its fitted values. Recall that:
$$X_i = \mu + \rho Z_i \Leftrightarrow \hat{X}_i + \eta_i$$
and,
$$\\hat{\beta}_{1(\text{IV})} = \frac{C(\hat{\rho} Z, Y)}{V(\hat{\rho} Z)}$$
Then the 2SLS estimator is:
$$\hat{\beta}_{1(\text{IV})} = \frac{C(\hat{\rho} Z, Y)}{V(\hat{\rho} Z)}$$
What this coefficient allows us to know is that:
$$C(\hat{X},Y)=E[\hat{X}Y]-E[\hat{X}]E[Y]$$
We can then substitute $\hat{X}=\hat{\mu} + \hat{\rho} Z_i$ into the above definition:
$$C(\hat{X},Y)=E[Y(\hat{\mu} + \hat{\rho} Z_i)]-E[Y]E[\hat{\mu} + \hat{\rho} Z_i]$$
We can then expand out the expectations:
$$E[Y(\hat{\mu} + \hat{\rho} Z_i)]=\hat{\mu}E[Y]+\hat{\rho}E[YZ]$$
$$E[\hat{\mu} + \hat{\rho} Z_i]=\hat{\mu}+\hat{\rho}E[Z]$$
Thus,
$$E[Y]E[\hat{\mu} + \hat{\rho} Z_i]=\hat{\mu}E[Y]+\hat{\rho}E[Y]E[Z]$$
Which we can then substitute back into the covariance formula:
$$C(\hat{X},Y)=[\hat{\mu}E[Y]+\hat{\rho}E[YZ]]-[\hat{\mu}E[Y]]+\hat{\rho}E[Y]E[Z]$$
We can then cancel out $\hat{\mu}E[Y]$:
$$C(\hat{X},Y)=\hat{\rho}(E[YZ]-E[Y]E[Z])$$
Since $E[YZ]-E[Y]E[Z]$ is the definition of covariance $C(Y,Z)$. Thus:
$$C(\hat{X},Y)=\hat{\rho}C(Y,Z)$$
Based on this derivation we can see that the covariance between $\hat{X}$ and $Y$ depends only on the coefficient $\hat{\rho}$ and the covariance between $Y$ and $Z$. The constant $\hat{\mu}$ drops out because it does not contribute to the variation. Since we want to know the causal relationship between $X$ (the endogenous regressor) and $Y$ (the dependent variable), using an instrument $Z$, we can see that the covariance of $\hat{X}$ and $Y$ depends on the covariance of $Y$ and $Z$ multiplied by $\hat{\rho}$. If $\hat{\rho}$ is the strength of the instrument $Z$ (i.e. reflects how much $Z$ explains $X$), then the larger $\hat{\rho}$ is then $Z$ explains more of the variation in $X$, which will result in a stronger relationship between $Y$ and $\hat{X}$ through the instrument. Essentially, a higher $\hat{\rho}$  leads to a more reliable IV estimate because it provides more information to estimate the causal effect of $X$ on $Y$.</p>
<h1 id="a-comment-on-iv-assumptions">A Comment on IV Assumptions</h1>
<hr />
<h3 id="good-instruments">Good Instruments</h3>
<p>Instrumental Variable Regression enables us to estimate the causal effect in the presence of <em>selection on an unobservable variable</em>, provided that we have access to <em>an additional observed variable</em> that fulfils specific criteria, known as an <strong>instrument</strong>. The instrument must meet three conditions:
    <ol>
        <li><strong>Exogeneity</strong>: The instrument must not be correlated with the confounding factors that introduce omitted variable bias.</li>
        <li><strong>Relevance</strong>: The instrument should be correlated with the treatment variable.</li>
        <li><strong>Exclusion</strong>: The instrument must not have a direct effect on the outcome variable.</p></li>
    </ol>
<p>However, as far as I am aware, there exists no <em>effective</em> way to determine whether the <strong>Exogeneity and Exclusion Restrictions</strong> have properly been met without additional assumptions, as they depend on the validity of the theoretical arguments. How big of an effect this has, I am not sure.</p>
<h3 id="homogeneity">Homogeneity</h3>
<p>I think it is important to note that, for the above explanations of Instrumental Variables, they rely on the idea that the treatment effects are homogenous for every unit. This homogeneity assumption implies that each unit responds to the treatment in the same way, leading to a uniform treatment effect. This is a somewhat unrealistic assumption, especially if one wanted to make policy implications.</p>
<p><a href="https://www.jstor.org/stable/2006780">Angrist (1990)</a>, does provide a framework for IV estimation with heterogenous treatment effects which he explores using the effect of military service on earnings, using a draft lottery as the instrumental variable. This new strategy requires the introduction of a fourth condition to our previous three: 1. Exogeneity, 2. Relevance, 3. Exclusion, and 4. Monotonicity. These four conditions constitute the "LATE" framework (Local Average Treatment Effect). </p>
<p>However, the problem with estimating IV for heterogenous agents is that it only works for "Compliers" and "Deifiers", and since "Deifiers" are essentially non-existent, we can argue that it realistically only works for "Compliers". This is problematic, as IV for heterogenous agents only allows us to estimate the treatment effect for a subsection of the population the "Compliers", which isn't entirely useful, especially when we are often interested in quantifying the treatment effect for the whole population. </p>
<h3 id="bias">Bias</h3>
<p><a href="https://arxiv.org/abs/1203.3503">Pearl (2012)</a> has provided an important criticism for Instrumental Variable regression, especially when applied to linear regression. Finding that, in linear models, including an Instrumental Variable invariably increases confounding bias (if it already exists). This is due to the relationship between the Instrumental Variable, the treatment, and the confounding variable. However, this bias doesn't necessarily occur in non-linear models, in these cases bias may increase, be introduced or reduced.</p>
<p>Supposing we have a treatment variable $X$, outcome variable $Y$ and some unobserved confounder $U$, where the instrumental variable $Z$ can be represented in the following two stage equations:
$$Y=c_0X+c_2U + \epsilon_Y$$
$$X=c_1U+c_3Z+\epsilon_X$$
Where:</p>
<ul>
<li>$c_0$ represents the causal effect of $X$ on $Y$</li>
<li>$c_1$ and $c_3$ represent the effects of $U$ and $Z$ on $X$, respectively.</li>
<li>$c_2$ represents the effect of $U$ on $Y$.</li>
<li>$\epsilon_Y$ and $\epsilon_X$ are error terms.</li>
</ul>
<p>There are three main quantities that we want to derive:
1. <strong>True Causal Effect</strong> ($A_1$): This is the direct causal relationship between $X$ and $Y$:
$$A_1=\frac{\partial}{\partial X}E[Y|{do}(X)]=c_0$$</p>
<div class="quote">
<p>Interventions and counterfactuals are defined through a mathematical operator called $do(x)$, which simulates physical interventions by deleting certain functions from the model, replacing them with a constant $X=x$, while keeping the rest of the model unchanged. <a href="https://ftp.cs.ucla.edu/pub/stat_ser/r402.pdf">Source</a></p>
</div>
<ol>
<li><strong>Unadjusted Estimate</strong> ($A_2$): This measures the naÃ¯ve dependence of $Y$ on $X$, combining the direct causal effect and bias due to $U$:
$$A_2=c_0+c_1\cdot c_2$$</li>
<li><strong>Conditioned Estimate</strong> ($A_3$): After conditioning on $Z$, the dependence of $Y$ on $X$ includes contributions from both $U$ and $Z$:
$$A_3=c_0 + c_2 \cdot \frac{c_1}{1-c_3^2}$$
We can quantify bias before and after conditioning on $Z$ as follows:
$$B_0=A_2-A_1=c_1 \cdot c_2$$
$$B_Z=A_3-A_1=\frac{c_1\cdot c_2}{1-c_3^2}$$
This means that:
$$B_Z=\frac{B_0}{1-c_2^3}$$
It is therefore clear that $\left| B_Z \right| \geq \left|B_0 \right|$ regardless of the signs $c_1$ and $c_2$, holding whenever $\left|B_0\right| &gt; 0$ and $c_3&gt;0$. This shows that if bias exists before IV ($B_0$), the bias is amplified by a factor inversely proportional to $1-c_3^2$, i.e. a factor of $\frac{1}{1-c_3^2}$. If $c_3$ (the effect of $Z$ on $X$) is close to $1$, the amplification is significant.</li>
</ol>
<p>Thus, the amplification effect arises because conditioning on $Z$ aligns $X$ more closely with $U$, exacerbating the influence of the unobserved confounder.</p>
<p>As the Pearl points out, this bias can be alleviated when using non-linear models. Suppose we take the same equations from above but apply non-linear functions:
$$Y=f(X)+U\cdot g(X)+\epsilon_Y$$
$$X=c_1U+c_3Z+\epsilon_X$$
Where, $f(X)$ and $g(U)$ are non-linear functions, and th dependence between $X$ and $Y$ or $U$ and $Y$ is no longer proportional. Apart from the obvious difference compared to linear models, where conditioning on $Z$ fully aligns $X$ with $U$, which leads to an amplification of the OVB. In non-linear systems the effects of $U$ and $Z$ on $Y$ and $X$ are modulated by the non-linear forms of $f$ and $g$, which <em>can</em> redistribute/reduce the bias. </p>
<p>We can thus quantify the bias before and after conditioning on $Z$ as follows:
$$B_0=E(Y|X)-E(Y|do(X))$$
$$B_Z=E(Y|X,Z)-E(Y|do(X))$$
Unlike linear models, the non-linear nature of $f$ and $g$ may result in cases where $\left|B_z\right|&lt;\left|B_0\right|$. </p>
<p>For example suppose our $f$ and $g$ functions are logarithmic and squared respectively, i.e.
$$Y=\log(X)+U^2+\epsilon_Y$$
$$X=c_3Z+c_1U+\epsilon_X$$
The bias quantifications become:
$$B_0=U^2$$
$$B_Z=\frac{U^2}{1-c^2_3}-\frac{\partial \log(X)}{\partial X}\cdot c_3$$
Thus, we can see that the term $\frac{\partial \log(X)}{\partial X}\cdot c_3$ can offset the $U^2$, reducing the overall bias after conditioning on $Z$.</p>
<p>However, again unlike linear models, it is possible linear models introduce a bias where none previously existed. This can occur if the functions of $f$ and/or $g$ create dependencies between $Z$ and $Y$ that were absent in the unconditioned model.</p>
<h3 id="my-further-thoughts">My Further Thoughts</h3>
<p>My knowledge of Instrumental Variables is what I would consider to be rudimentary at best, as such I would consider this post realistically a draft, and I aim to update it as I continue to learn about Instrumental Variables. I would appreciate feedback from those more familiar with advanced understandings of IV.</p>
<p>Some questions I have had are:</p>
<ul>
<li>Is inference actually possible with weak instruments?</li>
<li>Is there a place for linear models in IV, are non-parametric models inherently superior in the case of minimising bias?</li>
<li>Is the F-test effective for detecting weak instruments. I know for a fact an F-test won't tell you how good your model specification is, and by extension they can't really tell you that you have a good instrument.</li>
</ul>
                </div>
            </article>
        </main>
        <footer>
            <p>Queries, Quibbles, and Quotes: notquiteoptimal@gmail.com</p>
        </footer>
        <script>
            MathJax = {
                tex: {
                    inlineMath: [['$', '$']],
                    displayMath: [['$$', '$$']]
                }
            };
        </script>
    </body>
    </html>
    